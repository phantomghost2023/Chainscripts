#!/usr/bin/env python3
"""
Test script to validate the improved adaptive profiling and categorization logic
"""

from core.nano_engine import NanoScriptEngine

def test_script_categorization():
    """Test script size categorization with various script names"""
    engine = NanoScriptEngine()
    
    test_scripts = [
        # Small scripts (should be ‚â§50 lines)
        "quick_test.py",
        "small_demo.py", 
        "tiny_helper.py",
        "test_api.py",
        "demo_fetch.py",
        
        # Medium scripts (should be 51-150 lines)
        "medium_processor.py",
        "standard_cleaner.py",
        "api_handler.py",
        "process_data.py",
        "clean_csv.py",
        
        # Large scripts (should be >150 lines)
        "large_analyzer.py",
        "complex_transform.py",
        "big_processor.py",
        "heavy_computation.py",
        "analyze_dataset.py"
    ]
    
    print("Testing Enhanced Adaptive Script Profiling and Categorization")
    print("=" * 80)
    
    category_counts = {'small': 0, 'medium-lightweight': 0, 'medium-balanced': 0, 'large': 0}
    
    for script_name in test_scripts:
        profile = engine.analyze_script_complexity(f"nano_scripts/{script_name}")
        category = profile['size_category']
        line_count = profile['line_count']
        complexity = profile['complexity_score']
        
        # Update category counts based on the actual category returned
        if category in category_counts:
            category_counts[category] += 1
        else:
            # Handle cases where a new category might appear that wasn't initialized
            category_counts[category] = 1
        
        print(f"Script: {script_name:25} | Lines: {line_count:3d} | Category: {category:6} | Complexity: {complexity:6.1f}")
        
        # Display enhanced profiling metrics for first few scripts
        if script_name in test_scripts[:3]:  # Show detailed metrics for first 3 scripts
            print(f"  üìä AST Metrics: funcs={profile.get('ast_metrics', {}).get('functions', 0)}, "
                  f"classes={profile.get('ast_metrics', {}).get('classes', 0)}, "
                  f"imports={profile.get('ast_metrics', {}).get('imports', 0)}, "
                  f"loops={profile.get('ast_metrics', {}).get('loops', 0)}")
            
            patterns = profile.get('code_patterns', {})
            print(f"  üîç Code Patterns: io={patterns.get('io_operations', 0)}, "
                  f"network={patterns.get('network_calls', 0)}, "
                  f"db={patterns.get('database_calls', 0)}, "
                  f"cpu_intensive={patterns.get('cpu_intensive', False)}")
    
    print("\n" + "=" * 80)
    print("Categorization Summary:")
    print(f"Small scripts (‚â§85 lines):   {category_counts['small']:2d}")
    print(f"Medium-Lightweight scripts (86-150 lines): {category_counts['medium-lightweight']:2d}")
    print(f"Medium-Balanced scripts (151-300 lines): {category_counts['medium-balanced']:2d}")
    print(f"Large scripts (>300 lines):  {category_counts['large']:2d}")
    
    # Validate expected categories
    # Define expected categories for each script based on the new thresholds
    # Assuming quick_test.py, small_demo.py, tiny_helper.py, test_api.py, demo_fetch.py are small (<=85 lines)
    # Assuming medium_processor.py, standard_cleaner.py are medium-lightweight (86-150 lines)
    # Assuming api_handler.py, process_data.py, clean_csv.py are medium-balanced (151-300 lines)
    # Assuming large_analyzer.py, complex_transform.py, big_processor.py, heavy_computation.py, analyze_dataset.py are large (>300 lines)

    expected_categorization = {
        "quick_test.py": "small",
        "small_demo.py": "small",
        "tiny_helper.py": "small",
        "test_api.py": "small",
        "demo_fetch.py": "small",
        "medium_processor.py": "medium-lightweight",
        "standard_cleaner.py": "medium-lightweight",
        "api_handler.py": "medium-balanced",
        "process_data.py": "medium-balanced",
        "clean_csv.py": "medium-lightweight",
        "large_analyzer.py": "large",
        "complex_transform.py": "medium-balanced",
        "big_processor.py": "large",
        "heavy_computation.py": "large",
        "analyze_dataset.py": "large"
    }

    print("\nValidation Results:")
    validation_passed = True

    for script_name, expected_category in expected_categorization.items():
        profile = engine.analyze_script_complexity(f"nano_scripts/{script_name}")
        actual_category = profile['size_category']
        if actual_category != expected_category:
            print(f"‚ùå {script_name} expected '{expected_category}', got '{actual_category}'")
            validation_passed = False
    
    if validation_passed:
        print("‚úÖ All categorizations match expected patterns!")
    else:
        print("‚ö†Ô∏è  Some categorizations don't match expected patterns")
    
    return validation_passed

def test_adaptive_compilation():
    """Test adaptive compilation strategies"""
    engine = NanoScriptEngine()
    
    print("\n" + "=" * 60)
    print("Testing Adaptive Compilation Strategies")
    print("=" * 60)
    
    test_cases = [
        ("tiny_helper.py", "small"),
        ("clean_csv.py", "medium-lightweight"), 
        ("complex_transform.py", "medium-balanced")
    ]
    
    for script_name, expected_category in test_cases:
        print(f"\nTesting {script_name} (expected: {expected_category}):")
        
        # Analyze script
        profile = engine.analyze_script_complexity(f"nano_scripts/{script_name}")
        actual_category = profile['size_category']
        
        print(f"  Profile: {profile}")
        
        # Test compilation
        compiled_script = engine.load_script(script_name.replace('.py', ''))
        print(f"  Compiled: {compiled_script}")
        
        # Test execution
        execution_time = engine.execute_script(script_name.replace('.py', ''))
        print(f"  Execution time: {execution_time:.4f}s")
        
        if actual_category == expected_category:
            print(f"  ‚úÖ Category matches expected: {expected_category}")
        else:
            print(f"  ‚ùå Category mismatch: expected {expected_category}, got {actual_category}")

if __name__ == "__main__":
    print("ChainScript Adaptive Profiling Diagnostic Test")
    print("=" * 60)
    
    # Run tests
    categorization_passed = test_script_categorization()
    test_adaptive_compilation()
    
    print("\n" + "=" * 60)
    if categorization_passed:
        print("üéâ Adaptive profiling system is working correctly!")
        print("   - Script categorization is accurate")
        print("   - Compilation strategies are properly assigned")
        print("   - Ready for performance benchmarking")
    else:
        print("‚ö†Ô∏è  Adaptive profiling needs further adjustment")
        print("   - Some categorizations may not be optimal")
        print("   - Consider adjusting thresholds or simulation logic")
